{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 Group 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Import all relevant libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "import multiprocessing\n",
    "from scipy.io import loadmat\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier  #GBM algorithm\n",
    "from sklearn.decomposition import PCA\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from scipy.io import loadmat\n",
    "\n",
    "\n",
    "#import custom defined functions\n",
    "sys.path.append(\"..\")\n",
    "import lib.feature as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Initialize relevant files and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interactive setting\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "#training data labels\n",
    "info = pd.read_csv('../data/train_set/label.csv',usecols = range(1,6))\n",
    "\n",
    "#parameters\n",
    "rand_seed = 123\n",
    "test_size = 0.2\n",
    "cv_folds = 5\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "all_idx = np.array(info.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in all the points from matrices and construct data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readMat(index):\n",
    "    thisMat = loadmat('../data/train_set/points/' + '%04d' % index + '.mat')\n",
    "    return pd.DataFrame(round(pd.DataFrame(thisMat[list(thisMat)[3]]),0))\n",
    "\n",
    "fiducial_pt_list = list(map(readMat, list(range(1,2501))))\n",
    "#save and load file\n",
    "f = open('../output/fiducial_pt_list', 'wb')\n",
    "pickle.dump(fiducial_pt_list, f)\n",
    "f.close()\n",
    "f = open('../output/fiducial_pt_list', 'rb')\n",
    "fiducial_pt_list = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "dat_full = ft.feature(copy.deepcopy(fiducial_pt_list),all_idx,info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature5998</th>\n",
       "      <th>feature5999</th>\n",
       "      <th>feature6000</th>\n",
       "      <th>feature6001</th>\n",
       "      <th>feature6002</th>\n",
       "      <th>feature6003</th>\n",
       "      <th>feature6004</th>\n",
       "      <th>feature6005</th>\n",
       "      <th>feature6006</th>\n",
       "      <th>emotion_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>...</td>\n",
       "      <td>111.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>44.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>...</td>\n",
       "      <td>111.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>42.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>...</td>\n",
       "      <td>102.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>47.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>...</td>\n",
       "      <td>112.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>35.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>...</td>\n",
       "      <td>102.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 6007 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "0      45.0      28.0       6.0      16.0      37.0      22.0       1.0   \n",
       "1      44.0      27.0       5.0      19.0      36.0      20.0       2.0   \n",
       "2      42.0      24.0       3.0      18.0      33.0      15.0       5.0   \n",
       "3      47.0      27.0       7.0      15.0      31.0      13.0       8.0   \n",
       "4      35.0      22.0       1.0      19.0      36.0      19.0       0.0   \n",
       "\n",
       "   feature8  feature9  feature10  ...  feature5998  feature5999  feature6000  \\\n",
       "0      25.0     176.0      136.0  ...        111.0        168.0        225.0   \n",
       "1      24.0     170.0      134.0  ...        111.0        169.0        228.0   \n",
       "2      23.0     153.0      118.0  ...        102.0        156.0        211.0   \n",
       "3      29.0     166.0      126.0  ...        112.0        173.0        234.0   \n",
       "4      18.0     161.0      122.0  ...        102.0        157.0        211.0   \n",
       "\n",
       "   feature6001  feature6002  feature6003  feature6004  feature6005  \\\n",
       "0         56.0        113.0        170.0         57.0        114.0   \n",
       "1         58.0        116.0        175.0         58.0        117.0   \n",
       "2         53.0        107.0        162.0         54.0        109.0   \n",
       "3         60.0        121.0        182.0         61.0        122.0   \n",
       "4         52.0        107.0        161.0         55.0        109.0   \n",
       "\n",
       "   feature6006  emotion_idx  \n",
       "0         57.0            1  \n",
       "1         59.0            1  \n",
       "2         55.0            1  \n",
       "3         61.0            1  \n",
       "4         54.0            1  \n",
       "\n",
       "[5 rows x 6007 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting into X/y, training/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dat_full.iloc[:,:-1],dat_full.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=rand_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Model Training and Testing\n",
    "\n",
    "****All models are saved and loaded to reduce waiting time during presentation. Uncomment the code in appropriate sections and rerun if needed.***\n",
    "\n",
    "### Baseline Model: Boosted Decision Stumps \n",
    "\n",
    "Implemented using GradientBoostingClassifier() from sklearn.ensemble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base GBM Fit Time is: 2 minutes and 20 seconds. Accuracy is: 37.8%\n"
     ]
    }
   ],
   "source": [
    "#baseGBM = GradientBoostingClassifier(random_state=rand_seed,max_depth = 1)\n",
    "#startTime = time.time()\n",
    "#baseGBM.fit(X_train, y_train)\n",
    "#endTime= time.time()-startTime\n",
    "#baseGBM_prediction = baseGBM.predict(X_test)\n",
    "#baseGBM_accuracy = accuracy_score(y_test,baseGBM_prediction)\n",
    "#\n",
    "##save/load output\n",
    "#baseGBMOut = {'Model':baseGBM, 'accuracy':baseGBM_accuracy,'Time':endTime}\n",
    "#f = open('../output/baseGBMOut', 'wb')\n",
    "#pickle.dump(baseGBMOut, f)\n",
    "#f.close()\n",
    "\n",
    "f = open('../output/baseGBMOut', 'rb')\n",
    "baseGBMOut = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "print(\"Base GBM Fit Time is: \" + str(int(baseGBMOut['Time']//60)) + \" minutes and \" + str(round(baseGBMOut['Time'] % 60)) + \" seconds. Accuracy is: \" + \n",
    "      str(baseGBMOut['accuracy']*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit using entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseGBM_final = GradientBoostingClassifier(random_state=rand_seed,max_depth = 1)\n",
    "# baseGBM_final.fit(X, y)\n",
    "# f = open('../output/baseGBM_final', 'wb')\n",
    "# pickle.dump(baseGBM_final, f)\n",
    "# f.close()\n",
    "f = open('../output/baseGBM_final', 'rb')\n",
    "baseGBM_final = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****The paramter tuning process was took extremely long time and did not finish on time, so we only included the code. The tuned parameters in the following section were concluded based on tuning of smaller scales, trial-error, and intuitions.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Improved Model : Neural Net\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduce New Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the original feature: \n",
    "\n",
    "vertical distance and horizontal distance between any two points among those 78 fiducial points, \n",
    "\n",
    "We introduced new feature: \n",
    "\n",
    "the angle between any two-point vectors and x-axis. \n",
    "\n",
    "Therefore, we have 6006 + 3003 = 9009 features now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import matlab matrixs and calculate new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for mats and label.csv\n",
    "path = '../data/train_set/'\n",
    "train_pt_dir = path + \"points/\"\n",
    "train_label_path = path + \"label.csv\"\n",
    "label_df = pd.read_csv(train_label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matlab matrix and round to int\n",
    "index = ['{0:04}'.format(num) for num in range(1, 2501)]\n",
    "\n",
    "mats = []\n",
    "for ind in index:\n",
    "    temp = loadmat( train_pt_dir + ind + \".mat\")\n",
    "    mats.append(temp[[*temp][-1]])\n",
    "    \n",
    "mats = [mat.round() for mat in mats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original feature function 78*77\n",
    "def feature_distance(mat_list, nfidu = 78):\n",
    "    def pairwise_dist(vec):\n",
    "        a = pairwise_distances(vec.reshape(nfidu,1))\n",
    "        return(a[np.triu_indices(nfidu, k = 1)])\n",
    "    \n",
    "    def pairwise_dist_result(mat_list):\n",
    "        a = np.apply_along_axis(pairwise_dist, 0, mat_list)\n",
    "        return(a.flatten('F')) \n",
    "     \n",
    "    feature_mat = [pairwise_dist_result(mat) for mat in mat_list]   \n",
    "    return(np.vstack(feature_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dist_df = pd.DataFrame(feature_distance(mats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new feature: the degree angle between each vector and x-axis, range:0-90\n",
    "# 78*77/2\n",
    "def feature_slope(mat_list, nfidu = 78):\n",
    "    def pairwise_dist(vec):\n",
    "        a = pairwise_distances(vec.reshape(nfidu,1))\n",
    "        return(a[np.triu_indices(nfidu, k = 1)])\n",
    "    \n",
    "    def pairwise_dist_result(mat):\n",
    "        a = np.apply_along_axis(pairwise_dist, 0, mat)\n",
    "        return(np.rad2deg(np.arctan2(a[:,1], a[:,0]))) \n",
    "     \n",
    "    feature_mat = [pairwise_dist_result(mat) for mat in mat_list]   \n",
    "    return(np.vstack(feature_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_slope_df = pd.DataFrame(feature_slope(mats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_all_df = pd.concat([feature_dist_df, feature_slope_df], axis=1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a feature dataframe with 9009 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply PCA to reduce feature dimension**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the train set into 80%:20%, apply PCA to 80% train set to train the neural net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n"
     ]
    }
   ],
   "source": [
    "X, y = feature_all_df, label_df[\"emotion_idx\"]\n",
    "y = np.asarray(y - 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, np.asarray(y), test_size=0.2, random_state=123)\n",
    "\n",
    "pca = PCA(n_components=0.99, whiten=True)\n",
    "X_pca_train = pca.fit_transform(X_train)\n",
    "X_pca_test = pca.transform(X_test)\n",
    "print(len(X_pca_train[1]))\n",
    "pc_train_dim = len(X_pca_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find best tuning parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 1000)              122000    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 600)               600600    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 400)               240400    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 22)                8822      \n",
      "=================================================================\n",
      "Total params: 2,053,622\n",
      "Trainable params: 2,053,622\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2000 samples\n",
      "Epoch 1/15\n",
      "2000/2000 [==============================] - 1s 518us/sample - loss: 3.2087 - accuracy: 0.2445\n",
      "Epoch 2/15\n",
      "2000/2000 [==============================] - 1s 455us/sample - loss: 2.8887 - accuracy: 0.2955\n",
      "Epoch 3/15\n",
      "2000/2000 [==============================] - 1s 346us/sample - loss: 2.6483 - accuracy: 0.2900\n",
      "Epoch 4/15\n",
      "2000/2000 [==============================] - 1s 336us/sample - loss: 2.4277 - accuracy: 0.3560\n",
      "Epoch 5/15\n",
      "2000/2000 [==============================] - 1s 339us/sample - loss: 2.0931 - accuracy: 0.4955\n",
      "Epoch 6/15\n",
      "2000/2000 [==============================] - 1s 389us/sample - loss: 1.7980 - accuracy: 0.5685\n",
      "Epoch 7/15\n",
      "2000/2000 [==============================] - 1s 347us/sample - loss: 1.6617 - accuracy: 0.6010\n",
      "Epoch 8/15\n",
      "2000/2000 [==============================] - 1s 342us/sample - loss: 1.4611 - accuracy: 0.6740\n",
      "Epoch 9/15\n",
      "2000/2000 [==============================] - 1s 342us/sample - loss: 1.2678 - accuracy: 0.7505\n",
      "Epoch 10/15\n",
      "2000/2000 [==============================] - 1s 345us/sample - loss: 1.1269 - accuracy: 0.7990\n",
      "Epoch 11/15\n",
      "2000/2000 [==============================] - 1s 342us/sample - loss: 0.9691 - accuracy: 0.8555\n",
      "Epoch 12/15\n",
      "2000/2000 [==============================] - 1s 344us/sample - loss: 0.9453 - accuracy: 0.8630\n",
      "Epoch 13/15\n",
      "2000/2000 [==============================] - 1s 346us/sample - loss: 0.8526 - accuracy: 0.8980\n",
      "Epoch 14/15\n",
      "2000/2000 [==============================] - 1s 349us/sample - loss: 0.7930 - accuracy: 0.9155\n",
      "Epoch 15/15\n",
      "2000/2000 [==============================] - 1s 348us/sample - loss: 0.7465 - accuracy: 0.9310\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.492"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D, MaxPooling1D, Dropout, Reshape\n",
    "from tensorflow.keras.regularizers import l1,l2\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(1000, input_dim=pc_train_dim, activation='tanh',kernel_initializer = 'lecun_uniform',activity_regularizer=l1(0.001)))\n",
    "model2.add(Dense(600, activation='tanh',kernel_initializer = 'lecun_uniform'))\n",
    "model2.add(Dense(600, activation='tanh',kernel_initializer = 'lecun_uniform'))\n",
    "model2.add(Dense(600, activation='tanh',kernel_initializer = 'lecun_uniform'))\n",
    "model2.add(Dense(600, activation='tanh',kernel_initializer = 'lecun_uniform'))\n",
    "model2.add(Dense(400, activation='tanh',kernel_initializer = 'lecun_uniform'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(22, activation = 'sigmoid'))\n",
    "\n",
    "model2.summary()\n",
    "\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model2.fit(X_pca_train, y_train, epochs = 15)\n",
    "\n",
    "pred2 = model2.predict(X_pca_test)\n",
    "pred_index2 = np.argmax(pred2, axis = 1)\n",
    "accuracy2 = accuracy_score(y_test, pred_index2)\n",
    "accuracy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc_train_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit using the entire dataset: feature and the optimal tuning parameter; \n",
    "\n",
    "save the model parameter for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "X, y = feature_all_df, label_df[\"emotion_idx\"]\n",
    "#X, y = feature_dist_df, label_df[\"emotion_idx\"]\n",
    "y = np.asarray(y - 1)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.99, whiten=True)\n",
    "X_pca = pca.fit_transform(X)\n",
    "print(len(X_pca[1]))\n",
    "pc_dim = len(X_pca[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 1000)              124000    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 600)               600600    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 400)               240400    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 22)                8822      \n",
      "=================================================================\n",
      "Total params: 2,055,622\n",
      "Trainable params: 2,055,622\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2500 samples\n",
      "Epoch 1/15\n",
      "2500/2500 [==============================] - 1s 448us/sample - loss: 3.1874 - accuracy: 0.2372\n",
      "Epoch 2/15\n",
      "2500/2500 [==============================] - 1s 332us/sample - loss: 2.8971 - accuracy: 0.2784\n",
      "Epoch 3/15\n",
      "2500/2500 [==============================] - 1s 412us/sample - loss: 2.4792 - accuracy: 0.3596\n",
      "Epoch 4/15\n",
      "2500/2500 [==============================] - 1s 348us/sample - loss: 2.1116 - accuracy: 0.4804\n",
      "Epoch 5/15\n",
      "2500/2500 [==============================] - 1s 332us/sample - loss: 1.8697 - accuracy: 0.5520\n",
      "Epoch 6/15\n",
      "2500/2500 [==============================] - 1s 334us/sample - loss: 1.6978 - accuracy: 0.6052\n",
      "Epoch 7/15\n",
      "2500/2500 [==============================] - 1s 341us/sample - loss: 1.5402 - accuracy: 0.6504\n",
      "Epoch 8/15\n",
      "2500/2500 [==============================] - 1s 356us/sample - loss: 1.3496 - accuracy: 0.7120\n",
      "Epoch 9/15\n",
      "2500/2500 [==============================] - 1s 379us/sample - loss: 1.1697 - accuracy: 0.7724\n",
      "Epoch 10/15\n",
      "2500/2500 [==============================] - 1s 347us/sample - loss: 1.0583 - accuracy: 0.8112\n",
      "Epoch 11/15\n",
      "2500/2500 [==============================] - 1s 335us/sample - loss: 0.9874 - accuracy: 0.8448\n",
      "Epoch 12/15\n",
      "2500/2500 [==============================] - 1s 373us/sample - loss: 0.9033 - accuracy: 0.8708\n",
      "Epoch 13/15\n",
      "2500/2500 [==============================] - 1s 416us/sample - loss: 0.8744 - accuracy: 0.8784\n",
      "Epoch 14/15\n",
      "2500/2500 [==============================] - 1s 372us/sample - loss: 0.7663 - accuracy: 0.9276\n",
      "Epoch 15/15\n",
      "2500/2500 [==============================] - 1s 431us/sample - loss: 0.8662 - accuracy: 0.8992\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D, MaxPooling1D, Dropout, Reshape\n",
    "from tensorflow.keras.regularizers import l1,l2\n",
    "\n",
    "model_main = Sequential()\n",
    "model_main.add(Dense(1000, input_dim=pc_dim, activation='tanh',kernel_initializer = 'lecun_uniform',activity_regularizer=l1(0.001)))\n",
    "model_main.add(Dense(600, activation='tanh',kernel_initializer = 'lecun_uniform'))\n",
    "model_main.add(Dense(600, activation='tanh',kernel_initializer = 'lecun_uniform'))\n",
    "model_main.add(Dense(600, activation='tanh',kernel_initializer = 'lecun_uniform'))\n",
    "model_main.add(Dense(600, activation='tanh',kernel_initializer = 'lecun_uniform'))\n",
    "model_main.add(Dense(400, activation='tanh',kernel_initializer = 'lecun_uniform'))\n",
    "model_main.add(Dropout(0.2))\n",
    "model_main.add(Dense(22, activation = 'sigmoid'))\n",
    "\n",
    "model_main.summary()\n",
    "\n",
    "model_main.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model_main.fit(X_pca, y, epochs = 15)\n",
    "\n",
    "model_main.save(path + 'neural_network.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded = load_model(path + 'neural_network.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the model to the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pt_dir = \"../data/test_set/points/\"\n",
    "# import matlab matrix and round to int\n",
    "ntest_mat = 2500\n",
    "index = ['{0:04}'.format(num) for num in range(1, ntest_mat + 1)]\n",
    "\n",
    "test_mats = []\n",
    "for ind in index:\n",
    "    temp = loadmat( test_pt_dir + ind + \".mat\")\n",
    "    mats.append(temp[[*temp][-1]])\n",
    "    \n",
    "test_mats = [mat.round() for mat in test_mats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_feature_dist_df = pd.DataFrame(feature_distance(test_mats))\n",
    "t_feature_slope_df = pd.DataFrame(feature_slope(test_mats))\n",
    "t_feature_all_df = pd.concat([feature_dist_df, feature_slope_df], axis=1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX, ty = t_feature_all_df, label_df[\"emotion_idx\"]\n",
    "ty = np.asarray(ty - 1)\n",
    "\n",
    "tpca = PCA(n_components=0.99, whiten=True)\n",
    "tX_pca = tpca.fit_transform(tX)\n",
    "print(len(tX_pca[1]))\n",
    "tpc_dim = len(tX_pca[1])\n",
    "\n",
    "#tpred = loaded.predict(tX)\n",
    "#tpred_index = np.argmax(tpred, axis = 1) + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
