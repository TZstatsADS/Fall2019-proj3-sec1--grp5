{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 Group 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ting Cai, Yicheng Li, Wenyue Wu, Kangkang Zhang, Na Zhuo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Import all relevant libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/lightgbm/__init__.py:48: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "import multiprocessing\n",
    "from scipy.io import loadmat\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier  #GBM algorithm\n",
    "from sklearn.decomposition import PCA\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from scipy.io import loadmat\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D, MaxPooling1D, Dropout, Reshape\n",
    "from tensorflow.keras.regularizers import l1,l2\n",
    "\n",
    "\n",
    "#import custom defined functions\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import lib.feature as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Initialize relevant files and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interactive setting\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "#training data labels\n",
    "info = pd.read_csv('../data/train_set/label.csv',usecols = range(1,6))\n",
    "\n",
    "#parameters\n",
    "rand_seed = 123\n",
    "test_size = 0.2\n",
    "cv_folds = 5\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "all_idx = np.array(info.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in all the points from matrices and construct data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readMat(index):\n",
    "    thisMat = loadmat('../data/train_set/points/' + '%04d' % index + '.mat')\n",
    "    return pd.DataFrame(round(pd.DataFrame(thisMat[list(thisMat)[3]]),0))\n",
    "\n",
    "fiducial_pt_list = list(map(readMat, list(range(1,2501))))\n",
    "#save and load file\n",
    "f = open('../output/fiducial_pt_list', 'wb')\n",
    "pickle.dump(fiducial_pt_list, f)\n",
    "f.close()\n",
    "f = open('../output/fiducial_pt_list', 'rb')\n",
    "fiducial_pt_list = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "dat_full = ft.feature(copy.deepcopy(fiducial_pt_list),all_idx,info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature5998</th>\n",
       "      <th>feature5999</th>\n",
       "      <th>feature6000</th>\n",
       "      <th>feature6001</th>\n",
       "      <th>feature6002</th>\n",
       "      <th>feature6003</th>\n",
       "      <th>feature6004</th>\n",
       "      <th>feature6005</th>\n",
       "      <th>feature6006</th>\n",
       "      <th>emotion_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>...</td>\n",
       "      <td>111.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>44.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>...</td>\n",
       "      <td>111.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>42.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>...</td>\n",
       "      <td>102.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>47.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>...</td>\n",
       "      <td>112.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>35.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>...</td>\n",
       "      <td>102.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 6007 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "0      45.0      28.0       6.0      16.0      37.0      22.0       1.0   \n",
       "1      44.0      27.0       5.0      19.0      36.0      20.0       2.0   \n",
       "2      42.0      24.0       3.0      18.0      33.0      15.0       5.0   \n",
       "3      47.0      27.0       7.0      15.0      31.0      13.0       8.0   \n",
       "4      35.0      22.0       1.0      19.0      36.0      19.0       0.0   \n",
       "\n",
       "   feature8  feature9  feature10  ...  feature5998  feature5999  feature6000  \\\n",
       "0      25.0     176.0      136.0  ...        111.0        168.0        225.0   \n",
       "1      24.0     170.0      134.0  ...        111.0        169.0        228.0   \n",
       "2      23.0     153.0      118.0  ...        102.0        156.0        211.0   \n",
       "3      29.0     166.0      126.0  ...        112.0        173.0        234.0   \n",
       "4      18.0     161.0      122.0  ...        102.0        157.0        211.0   \n",
       "\n",
       "   feature6001  feature6002  feature6003  feature6004  feature6005  \\\n",
       "0         56.0        113.0        170.0         57.0        114.0   \n",
       "1         58.0        116.0        175.0         58.0        117.0   \n",
       "2         53.0        107.0        162.0         54.0        109.0   \n",
       "3         60.0        121.0        182.0         61.0        122.0   \n",
       "4         52.0        107.0        161.0         55.0        109.0   \n",
       "\n",
       "   feature6006  emotion_idx  \n",
       "0         57.0            1  \n",
       "1         59.0            1  \n",
       "2         55.0            1  \n",
       "3         61.0            1  \n",
       "4         54.0            1  \n",
       "\n",
       "[5 rows x 6007 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting into X/y, training/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dat_full.iloc[:,:-1],dat_full.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=rand_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Model Training and Testing\n",
    "\n",
    "****All models are saved and loaded to reduce waiting time during presentation. Uncomment the code in appropriate sections and rerun if needed.***\n",
    "\n",
    "### Baseline Model: Boosted Decision Stumps \n",
    "\n",
    "Implemented using GradientBoostingClassifier() from sklearn.ensemble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base GBM Fit Time is: 2 minutes and 20 seconds. Accuracy is: 37.8%\n"
     ]
    }
   ],
   "source": [
    "#baseGBM = GradientBoostingClassifier(random_state=rand_seed,max_depth = 1)\n",
    "#startTime = time.time()\n",
    "#baseGBM.fit(X_train, y_train)\n",
    "#endTime= time.time()-startTime\n",
    "#baseGBM_prediction = baseGBM.predict(X_test)\n",
    "#baseGBM_accuracy = accuracy_score(y_test,baseGBM_prediction)\n",
    "#\n",
    "##save/load output\n",
    "#baseGBMOut = {'Model':baseGBM, 'accuracy':baseGBM_accuracy,'Time':endTime}\n",
    "#f = open('../output/baseGBMOut', 'wb')\n",
    "#pickle.dump(baseGBMOut, f)\n",
    "#f.close()\n",
    "\n",
    "f = open('../output/baseGBMOut', 'rb')\n",
    "baseGBMOut = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "print(\"Base GBM Fit Time is: \" + str(int(baseGBMOut['Time']//60)) + \" minutes and \" + str(round(baseGBMOut['Time'] % 60)) + \" seconds. Accuracy is: \" + \n",
    "      str(baseGBMOut['accuracy']*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit using entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseGBM_final = GradientBoostingClassifier(random_state=rand_seed,max_depth = 1)\n",
    "# baseGBM_final.fit(X, y)\n",
    "# f = open('../output/baseGBM_final', 'wb')\n",
    "# pickle.dump(baseGBM_final, f)\n",
    "# f.close()\n",
    "f = open('../output/baseGBM_final', 'rb')\n",
    "baseGBM_final = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****The paramter tuning process was took extremely long time and did not finish on time, so we only included the code. The tuned parameters in the following section were concluded based on tuning of smaller scales, trial-error, and intuitions.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Running Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>GBM</td>\n",
       "      <td>37.8%</td>\n",
       "      <td>2m2s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>SVM (linear)</td>\n",
       "      <td>50.2%</td>\n",
       "      <td>40s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>SVM (poly)</td>\n",
       "      <td>48.0%</td>\n",
       "      <td>40s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>SVM (rbf)</td>\n",
       "      <td>47.6%</td>\n",
       "      <td>2s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Xgboost</td>\n",
       "      <td>51.0%</td>\n",
       "      <td>1m5s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Neural Net</td>\n",
       "      <td>51.2%</td>\n",
       "      <td>19s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>CNN</td>\n",
       "      <td>24.0%</td>\n",
       "      <td>21s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>41.0%</td>\n",
       "      <td>17s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model Accuracy Running Time\n",
       "0            GBM    37.8%         2m2s\n",
       "1   SVM (linear)    50.2%          40s\n",
       "2     SVM (poly)    48.0%          40s\n",
       "3      SVM (rbf)    47.6%           2s\n",
       "4        Xgboost    51.0%         1m5s\n",
       "5     Neural Net    51.2%          19s\n",
       "6            CNN    24.0%          21s\n",
       "7  Random Forest    41.0%          17s"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'Model':['GBM','SVM (linear)', 'SVM (poly)', 'SVM (rbf)', 'Xgboost', 'Neural Net', 'CNN','Random Forest'], \n",
    "        'Accuracy':['37.8%', '50.2%', '48.0%', '47.6%', '51.0%', '51.2%', '24.0%','41.0%'],\n",
    "        'Running Time':['2m2s', '40s', '40s','2s', '1m5s', '19s', '21s', '17s']} \n",
    "\n",
    "model_Select= pd.DataFrame(data)\n",
    "model_Select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Improved Model : Neural Net\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduce New Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the original feature: \n",
    "\n",
    "vertical distance and horizontal distance between any two points among those 78 fiducial points, \n",
    "\n",
    "We introduced new feature: \n",
    "\n",
    "the angle between any two-point vectors and x-axis. \n",
    "\n",
    "Therefore, we have 6006 + 3003 = 9009 features now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import matlab matrixs and calculate new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for mats and label.csv\n",
    "path = '../data/train_set/'\n",
    "train_pt_dir = path + \"points/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matlab matrix and round to int\n",
    "index = ['{0:04}'.format(num) for num in range(1, 2501)]\n",
    "\n",
    "mats = []\n",
    "for ind in index:\n",
    "    temp = loadmat( train_pt_dir + ind + \".mat\")\n",
    "    mats.append(temp[[*temp][-1]])\n",
    "    \n",
    "mats = [mat.round() for mat in mats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dist_df = pd.DataFrame(ft.feature_distance(mats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_slope_df = pd.DataFrame(ft.feature_slope(mats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_all_df = pd.concat([feature_dist_df, feature_slope_df], axis=1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a feature dataframe with 9009 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply PCA to reduce feature dimension**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the train set into 80%:20%, apply PCA to 80% train set to train the neural net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n"
     ]
    }
   ],
   "source": [
    "X, y = feature_all_df, dat_full[\"emotion_idx\"]\n",
    "y = np.asarray(y - 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, np.asarray(y), test_size=0.2, random_state=123)\n",
    "\n",
    "pca = PCA(n_components=0.99, whiten=True)\n",
    "X_pca_train = pca.fit_transform(X_train)\n",
    "X_pca_test = pca.transform(X_test)\n",
    "print(len(X_pca_train[1]))\n",
    "pc_train_dim = len(X_pca_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find best tuning parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1000)              122000    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 600)               600600    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 400)               240400    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 22)                8822      \n",
      "=================================================================\n",
      "Total params: 2,053,622\n",
      "Trainable params: 2,053,622\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2000 samples\n",
      "Epoch 1/15\n",
      "2000/2000 [==============================] - 1s 543us/sample - loss: 3.1910 - accuracy: 0.2380\n",
      "Epoch 2/15\n",
      "2000/2000 [==============================] - 1s 321us/sample - loss: 2.8169 - accuracy: 0.3145\n",
      "Epoch 3/15\n",
      "2000/2000 [==============================] - 1s 368us/sample - loss: 2.4769 - accuracy: 0.3745\n",
      "Epoch 4/15\n",
      "2000/2000 [==============================] - 1s 319us/sample - loss: 2.1119 - accuracy: 0.4755\n",
      "Epoch 5/15\n",
      "2000/2000 [==============================] - 1s 386us/sample - loss: 1.8447 - accuracy: 0.5640\n",
      "Epoch 6/15\n",
      "2000/2000 [==============================] - 1s 409us/sample - loss: 1.6407 - accuracy: 0.6340\n",
      "Epoch 7/15\n",
      "2000/2000 [==============================] - 1s 323us/sample - loss: 1.4571 - accuracy: 0.6955\n",
      "Epoch 8/15\n",
      "2000/2000 [==============================] - 1s 362us/sample - loss: 1.2322 - accuracy: 0.7640 - loss: 1.2\n",
      "Epoch 9/15\n",
      "2000/2000 [==============================] - 1s 326us/sample - loss: 1.0959 - accuracy: 0.8175\n",
      "Epoch 10/15\n",
      "2000/2000 [==============================] - 1s 314us/sample - loss: 0.9546 - accuracy: 0.8585\n",
      "Epoch 11/15\n",
      "2000/2000 [==============================] - 1s 310us/sample - loss: 0.8842 - accuracy: 0.8825\n",
      "Epoch 12/15\n",
      "2000/2000 [==============================] - 1s 310us/sample - loss: 0.8212 - accuracy: 0.9015\n",
      "Epoch 13/15\n",
      "2000/2000 [==============================] - 1s 312us/sample - loss: 0.7936 - accuracy: 0.9285\n",
      "Epoch 14/15\n",
      "2000/2000 [==============================] - 1s 316us/sample - loss: 0.7140 - accuracy: 0.9495\n",
      "Epoch 15/15\n",
      "2000/2000 [==============================] - 1s 319us/sample - loss: 0.7083 - accuracy: 0.9530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a3c6af710>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.512"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(1000, input_dim=pc_train_dim, activation='tanh',kernel_initializer = 'lecun_uniform',activity_regularizer=l1(0.001)))\n",
    "model2.add(Dense(600, activation='tanh',kernel_initializer = 'lecun_uniform'))\n",
    "model2.add(Dense(600, activation='tanh',kernel_initializer = 'lecun_uniform'))\n",
    "model2.add(Dense(600, activation='tanh',kernel_initializer = 'lecun_uniform'))\n",
    "model2.add(Dense(600, activation='tanh',kernel_initializer = 'lecun_uniform'))\n",
    "model2.add(Dense(400, activation='tanh',kernel_initializer = 'lecun_uniform'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(22, activation = 'sigmoid'))\n",
    "\n",
    "model2.summary()\n",
    "\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model2.fit(X_pca_train, y_train, epochs = 15)\n",
    "\n",
    "pred2 = model2.predict(X_pca_test)\n",
    "pred_index2 = np.argmax(pred2, axis = 1)\n",
    "accuracy2 = accuracy_score(y_test, pred_index2)\n",
    "accuracy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit using the entire dataset: feature and the optimal tuning parameter; \n",
    "\n",
    "save the model parameter for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "X, y = feature_all_df, dat_full[\"emotion_idx\"]\n",
    "#X, y = feature_dist_df, label_df[\"emotion_idx\"]\n",
    "y = np.asarray(y - 1)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.99, whiten=True)\n",
    "X_pca = pca.fit_transform(X)\n",
    "print(len(X_pca[1]))\n",
    "pc_dim = len(X_pca[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 1000)              124000    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 600)               600600    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 400)               240400    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 22)                8822      \n",
      "=================================================================\n",
      "Total params: 2,055,622\n",
      "Trainable params: 2,055,622\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2500 samples\n",
      "Epoch 1/15\n",
      "2500/2500 [==============================] - 1s 496us/sample - loss: 3.1721 - accuracy: 0.2360\n",
      "Epoch 2/15\n",
      "2500/2500 [==============================] - 1s 336us/sample - loss: 2.7713 - accuracy: 0.3124\n",
      "Epoch 3/15\n",
      "2500/2500 [==============================] - 1s 330us/sample - loss: 2.5244 - accuracy: 0.3232\n",
      "Epoch 4/15\n",
      "2500/2500 [==============================] - 1s 377us/sample - loss: 2.2064 - accuracy: 0.4316\n",
      "Epoch 5/15\n",
      "2500/2500 [==============================] - 1s 375us/sample - loss: 1.9021 - accuracy: 0.5352\n",
      "Epoch 6/15\n",
      "2500/2500 [==============================] - 1s 375us/sample - loss: 1.7801 - accuracy: 0.5808\n",
      "Epoch 7/15\n",
      "2500/2500 [==============================] - 1s 328us/sample - loss: 1.5633 - accuracy: 0.6496\n",
      "Epoch 8/15\n",
      "2500/2500 [==============================] - 1s 369us/sample - loss: 1.4586 - accuracy: 0.6812\n",
      "Epoch 9/15\n",
      "2500/2500 [==============================] - 1s 367us/sample - loss: 1.3001 - accuracy: 0.7344\n",
      "Epoch 10/15\n",
      "2500/2500 [==============================] - 1s 364us/sample - loss: 1.1554 - accuracy: 0.7864\n",
      "Epoch 11/15\n",
      "2500/2500 [==============================] - 1s 334us/sample - loss: 1.0359 - accuracy: 0.8264\n",
      "Epoch 12/15\n",
      "2500/2500 [==============================] - 1s 324us/sample - loss: 0.9558 - accuracy: 0.8528\n",
      "Epoch 13/15\n",
      "2500/2500 [==============================] - 1s 327us/sample - loss: 0.9185 - accuracy: 0.8680\n",
      "Epoch 14/15\n",
      "2500/2500 [==============================] - 1s 365us/sample - loss: 0.8765 - accuracy: 0.8800\n",
      "Epoch 15/15\n",
      "2500/2500 [==============================] - 1s 332us/sample - loss: 0.8095 - accuracy: 0.9096\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a3d735f90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_main = Sequential()\n",
    "model_main.add(Dense(1000, input_dim=pc_dim, activation='tanh',kernel_initializer = 'lecun_uniform',activity_regularizer=l1(0.001)))\n",
    "model_main.add(Dense(600, activation='tanh',kernel_initializer = 'lecun_uniform'))\n",
    "model_main.add(Dense(600, activation='tanh',kernel_initializer = 'lecun_uniform'))\n",
    "model_main.add(Dense(600, activation='tanh',kernel_initializer = 'lecun_uniform'))\n",
    "model_main.add(Dense(600, activation='tanh',kernel_initializer = 'lecun_uniform'))\n",
    "model_main.add(Dense(400, activation='tanh',kernel_initializer = 'lecun_uniform'))\n",
    "model_main.add(Dropout(0.2))\n",
    "model_main.add(Dense(22, activation = 'sigmoid'))\n",
    "\n",
    "model_main.summary()\n",
    "\n",
    "model_main.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model_main.fit(X_pca, y, epochs = 15)\n",
    "\n",
    "model_main.save('../output/neural_network.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = load_model(path + 'neural_network.h5')\n",
    "df = pd.read_csv('../data/test_set_sec1/labels_prediction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the model to the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pt_dir = \"../data/test_set_sec1/points/\"\n",
    "# import matlab matrix and round to int\n",
    "ntest_mat = 2500\n",
    "index = ['{0:04}'.format(num) for num in range(1, ntest_mat + 1)]\n",
    "\n",
    "test_mats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in index:\n",
    "    temp = loadmat( test_pt_dir + ind + \".mat\")\n",
    "    test_mats.append(temp[[*temp][-1]])\n",
    "    \n",
    "test_mats = [mat.round() for mat in test_mats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_feature_dist_df = pd.DataFrame(ft.feature_distance(test_mats))\n",
    "t_feature_slope_df = pd.DataFrame(ft.feature_slope(test_mats))\n",
    "t_feature_all_df = pd.concat([t_feature_dist_df, t_feature_slope_df], axis=1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "baseline model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pred = baseGBM_final.predict(t_feature_dist_df)\n",
    "df['Baseline'] = base_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "advanced model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "tX = t_feature_all_df\n",
    "\n",
    "#tpca = PCA(n_components=0.99, whiten=True)\n",
    "tX_pca = pca.transform(tX)\n",
    "print(len(tX_pca[1]))\n",
    "tpc_dim = len(tX_pca[1])\n",
    "\n",
    "#tpred = loaded.predict(tX)\n",
    "#tpred_index = np.argmax(tpred, axis = 1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpred = loaded.predict(tX_pca)\n",
    "tpred_index = np.argmax(tpred, axis = 1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Advanced'] = tpred_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/test_set_sec1/labels_prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
