{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 Group 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ting Cai, Yicheng Li, Wenyue Wu, Kangkang Zhang, Na Zhuo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Import all relevant libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "import multiprocessing\n",
    "from scipy.io import loadmat\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier  #GBM algorithm\n",
    "from sklearn.decomposition import PCA\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from scipy.io import loadmat\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D, MaxPooling1D, Dropout, Reshape\n",
    "from tensorflow.keras.regularizers import l1,l2\n",
    "\n",
    "\n",
    "#import custom defined functions\n",
    "sys.path.append(\"..\")\n",
    "import lib.feature as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Initialize relevant files and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interactive setting\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "#training data labels\n",
    "info = pd.read_csv('../data/train_set/label.csv',usecols = range(1,6))\n",
    "\n",
    "#parameters\n",
    "rand_seed = 123\n",
    "test_size = 0.2\n",
    "cv_folds = 5\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "all_idx = np.array(info.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in all the points from matrices and construct data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readMat(index):\n",
    "    thisMat = loadmat('../data/train_set/points/' + '%04d' % index + '.mat')\n",
    "    return pd.DataFrame(round(pd.DataFrame(thisMat[list(thisMat)[3]]),0))\n",
    "\n",
    "fiducial_pt_list = list(map(readMat, list(range(1,2501))))\n",
    "#save and load file\n",
    "f = open('../output/fiducial_pt_list', 'wb')\n",
    "pickle.dump(fiducial_pt_list, f)\n",
    "f.close()\n",
    "f = open('../output/fiducial_pt_list', 'rb')\n",
    "fiducial_pt_list = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "dat_full = ft.feature(copy.deepcopy(fiducial_pt_list),all_idx,info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature5998</th>\n",
       "      <th>feature5999</th>\n",
       "      <th>feature6000</th>\n",
       "      <th>feature6001</th>\n",
       "      <th>feature6002</th>\n",
       "      <th>feature6003</th>\n",
       "      <th>feature6004</th>\n",
       "      <th>feature6005</th>\n",
       "      <th>feature6006</th>\n",
       "      <th>emotion_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>...</td>\n",
       "      <td>111.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>44.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>...</td>\n",
       "      <td>111.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>42.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>...</td>\n",
       "      <td>102.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>47.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>...</td>\n",
       "      <td>112.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>35.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>...</td>\n",
       "      <td>102.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 6007 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "0      45.0      28.0       6.0      16.0      37.0      22.0       1.0   \n",
       "1      44.0      27.0       5.0      19.0      36.0      20.0       2.0   \n",
       "2      42.0      24.0       3.0      18.0      33.0      15.0       5.0   \n",
       "3      47.0      27.0       7.0      15.0      31.0      13.0       8.0   \n",
       "4      35.0      22.0       1.0      19.0      36.0      19.0       0.0   \n",
       "\n",
       "   feature8  feature9  feature10  ...  feature5998  feature5999  feature6000  \\\n",
       "0      25.0     176.0      136.0  ...        111.0        168.0        225.0   \n",
       "1      24.0     170.0      134.0  ...        111.0        169.0        228.0   \n",
       "2      23.0     153.0      118.0  ...        102.0        156.0        211.0   \n",
       "3      29.0     166.0      126.0  ...        112.0        173.0        234.0   \n",
       "4      18.0     161.0      122.0  ...        102.0        157.0        211.0   \n",
       "\n",
       "   feature6001  feature6002  feature6003  feature6004  feature6005  \\\n",
       "0         56.0        113.0        170.0         57.0        114.0   \n",
       "1         58.0        116.0        175.0         58.0        117.0   \n",
       "2         53.0        107.0        162.0         54.0        109.0   \n",
       "3         60.0        121.0        182.0         61.0        122.0   \n",
       "4         52.0        107.0        161.0         55.0        109.0   \n",
       "\n",
       "   feature6006  emotion_idx  \n",
       "0         57.0            1  \n",
       "1         59.0            1  \n",
       "2         55.0            1  \n",
       "3         61.0            1  \n",
       "4         54.0            1  \n",
       "\n",
       "[5 rows x 6007 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting into X/y, training/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dat_full.iloc[:,:-1],dat_full.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=rand_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Model Training and Testing\n",
    "\n",
    "****All models are saved and loaded to reduce waiting time during presentation. Uncomment the code in appropriate sections and rerun if needed.***\n",
    "\n",
    "### Baseline Model: Boosted Decision Stumps \n",
    "\n",
    "Implemented using GradientBoostingClassifier() from sklearn.ensemble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base GBM Fit Time is: 2 minutes and 20 seconds. Accuracy is: 37.8%\n"
     ]
    }
   ],
   "source": [
    "#baseGBM = GradientBoostingClassifier(random_state=rand_seed,max_depth = 1)\n",
    "#startTime = time.time()\n",
    "#baseGBM.fit(X_train, y_train)\n",
    "#endTime= time.time()-startTime\n",
    "#baseGBM_prediction = baseGBM.predict(X_test)\n",
    "#baseGBM_accuracy = accuracy_score(y_test,baseGBM_prediction)\n",
    "#\n",
    "##save/load output\n",
    "#baseGBMOut = {'Model':baseGBM, 'accuracy':baseGBM_accuracy,'Time':endTime}\n",
    "#f = open('../output/baseGBMOut', 'wb')\n",
    "#pickle.dump(baseGBMOut, f)\n",
    "#f.close()\n",
    "\n",
    "f = open('../output/baseGBMOut', 'rb')\n",
    "baseGBMOut = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "print(\"Base GBM Fit Time is: \" + str(int(baseGBMOut['Time']//60)) + \" minutes and \" + str(round(baseGBMOut['Time'] % 60)) + \" seconds. Accuracy is: \" + \n",
    "      str(baseGBMOut['accuracy']*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit using entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseGBM_final = GradientBoostingClassifier(random_state=rand_seed,max_depth = 1)\n",
    "# baseGBM_final.fit(X, y)\n",
    "# f = open('../output/baseGBM_final', 'wb')\n",
    "# pickle.dump(baseGBM_final, f)\n",
    "# f.close()\n",
    "f = open('../output/baseGBM_final', 'rb')\n",
    "baseGBM_final = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****The paramter tuning process was took extremely long time and did not finish on time, so we only included the code. The tuned parameters in the following section were concluded based on tuning of smaller scales, trial-error, and intuitions.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Running Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>GBM</td>\n",
       "      <td>37.8%</td>\n",
       "      <td>2m2s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>SVM (linear)</td>\n",
       "      <td>50.2%</td>\n",
       "      <td>40s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>SVM (poly)</td>\n",
       "      <td>48.0%</td>\n",
       "      <td>40s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>SVM (rbf)</td>\n",
       "      <td>47.6%</td>\n",
       "      <td>2s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Xgboost</td>\n",
       "      <td>51.0%</td>\n",
       "      <td>1m5s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Neural Net</td>\n",
       "      <td>51.2%</td>\n",
       "      <td>19s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>CNN</td>\n",
       "      <td>24.0%</td>\n",
       "      <td>21s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>41.0%</td>\n",
       "      <td>17s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model Accuracy Running Time\n",
       "0            GBM    37.8%         2m2s\n",
       "1   SVM (linear)    50.2%          40s\n",
       "2     SVM (poly)    48.0%          40s\n",
       "3      SVM (rbf)    47.6%           2s\n",
       "4        Xgboost    51.0%         1m5s\n",
       "5     Neural Net    51.2%          19s\n",
       "6            CNN    24.0%          21s\n",
       "7  Random Forest    41.0%          17s"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'Model':['GBM','SVM (linear)', 'SVM (poly)', 'SVM (rbf)', 'Xgboost', 'Neural Net', 'CNN','Random Forest'], \n",
    "        'Accuracy':['37.8%', '50.2%', '48.0%', '47.6%', '51.0%', '51.2%', '24.0%','41.0%'],\n",
    "        'Running Time':['2m2s', '40s', '40s','2s', '1m5s', '19s', '21s', '17s']} \n",
    "\n",
    "model_Select= pd.DataFrame(data)\n",
    "model_Select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Improved Model : Neural Net\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduce New Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the original feature: \n",
    "\n",
    "vertical distance and horizontal distance between any two points among those 78 fiducial points, \n",
    "\n",
    "We introduced new feature: \n",
    "\n",
    "the angle between any two-point vectors and x-axis. \n",
    "\n",
    "Therefore, we have 6006 + 3003 = 9009 features now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import matlab matrixs and calculate new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for mats and label.csv\n",
    "path = '../data/train_set/'\n",
    "train_pt_dir = path + \"points/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matlab matrix and round to int\n",
    "index = ['{0:04}'.format(num) for num in range(1, 2501)]\n",
    "\n",
    "mats = []\n",
    "for ind in index:\n",
    "    temp = loadmat( train_pt_dir + ind + \".mat\")\n",
    "    mats.append(temp[[*temp][-1]])\n",
    "    \n",
    "mats = [mat.round() for mat in mats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dist_df = pd.DataFrame(feature_distance(mats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_slope_df = pd.DataFrame(feature_slope(mats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_all_df = pd.concat([feature_dist_df, feature_slope_df], axis=1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a feature dataframe with 9009 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply PCA to reduce feature dimension**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the train set into 80%:20%, apply PCA to 80% train set to train the neural net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n"
     ]
    }
   ],
   "source": [
    "X, y = feature_all_df, dat_full[\"emotion_idx\"]\n",
    "y = np.asarray(y - 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, np.asarray(y), test_size=0.2, random_state=123)\n",
    "\n",
    "pca = PCA(n_components=0.99, whiten=True)\n",
    "X_pca_train = pca.fit_transform(X_train)\n",
    "X_pca_test = pca.transform(X_test)\n",
    "print(len(X_pca_train[1]))\n",
    "pc_train_dim = len(X_pca_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find best tuning parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_71 (Dense)             (None, 1000)              122000    \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 600)               600600    \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 400)               240400    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 22)                8822      \n",
      "=================================================================\n",
      "Total params: 2,053,622\n",
      "Trainable params: 2,053,622\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2000 samples\n",
      "Epoch 1/15\n",
      "2000/2000 [==============================] - 1s 546us/sample - loss: 3.2066 - accuracy: 0.2310\n",
      "Epoch 2/15\n",
      "2000/2000 [==============================] - 1s 370us/sample - loss: 2.9098 - accuracy: 0.2720\n",
      "Epoch 3/15\n",
      "2000/2000 [==============================] - 1s 343us/sample - loss: 2.6572 - accuracy: 0.3185\n",
      "Epoch 4/15\n",
      "2000/2000 [==============================] - 1s 326us/sample - loss: 2.3464 - accuracy: 0.4005\n",
      "Epoch 5/15\n",
      "2000/2000 [==============================] - 1s 335us/sample - loss: 2.0505 - accuracy: 0.5015\n",
      "Epoch 6/15\n",
      "2000/2000 [==============================] - 1s 341us/sample - loss: 1.7975 - accuracy: 0.5740\n",
      "Epoch 7/15\n",
      "2000/2000 [==============================] - 1s 359us/sample - loss: 1.5680 - accuracy: 0.6355\n",
      "Epoch 8/15\n",
      "2000/2000 [==============================] - 1s 371us/sample - loss: 1.4548 - accuracy: 0.6735\n",
      "Epoch 9/15\n",
      "2000/2000 [==============================] - 1s 331us/sample - loss: 1.2330 - accuracy: 0.7570\n",
      "Epoch 10/15\n",
      "2000/2000 [==============================] - 1s 312us/sample - loss: 1.0830 - accuracy: 0.8075\n",
      "Epoch 11/15\n",
      "2000/2000 [==============================] - 1s 319us/sample - loss: 0.9839 - accuracy: 0.8490\n",
      "Epoch 12/15\n",
      "2000/2000 [==============================] - 1s 329us/sample - loss: 0.8870 - accuracy: 0.8820\n",
      "Epoch 13/15\n",
      "2000/2000 [==============================] - 1s 333us/sample - loss: 0.8048 - accuracy: 0.9150\n",
      "Epoch 14/15\n",
      "2000/2000 [==============================] - 1s 328us/sample - loss: 0.7453 - accuracy: 0.9335\n",
      "Epoch 15/15\n",
      "2000/2000 [==============================] - 1s 328us/sample - loss: 0.7058 - accuracy: 0.9470\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x115f15690>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.492"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(1000, input_dim=pc_train_dim, activation='tanh',kernel_initializer = 'lecun_uniform',activity_regularizer=l1(0.001)))\n",
    "model2.add(Dense(600, activation='tanh',kernel_initializer = 'lecun_uniform'))\n",
    "model2.add(Dense(600, activation='tanh',kernel_initializer = 'lecun_uniform'))\n",
    "model2.add(Dense(600, activation='tanh',kernel_initializer = 'lecun_uniform'))\n",
    "model2.add(Dense(600, activation='tanh',kernel_initializer = 'lecun_uniform'))\n",
    "model2.add(Dense(400, activation='tanh',kernel_initializer = 'lecun_uniform'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(22, activation = 'sigmoid'))\n",
    "\n",
    "model2.summary()\n",
    "\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model2.fit(X_pca_train, y_train, epochs = 15)\n",
    "\n",
    "pred2 = model2.predict(X_pca_test)\n",
    "pred_index2 = np.argmax(pred2, axis = 1)\n",
    "accuracy2 = accuracy_score(y_test, pred_index2)\n",
    "accuracy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit using the entire dataset: feature and the optimal tuning parameter; \n",
    "\n",
    "save the model parameter for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "X, y = feature_all_df, dat_full[\"emotion_idx\"]\n",
    "#X, y = feature_dist_df, label_df[\"emotion_idx\"]\n",
    "y = np.asarray(y - 1)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.99, whiten=True)\n",
    "X_pca = pca.fit_transform(X)\n",
    "print(len(X_pca[1]))\n",
    "pc_dim = len(X_pca[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_85 (Dense)             (None, 1000)              124000    \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 600)               600600    \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 400)               240400    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 22)                8822      \n",
      "=================================================================\n",
      "Total params: 2,055,622\n",
      "Trainable params: 2,055,622\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2500 samples\n",
      "Epoch 1/15\n",
      "2500/2500 [==============================] - 1s 506us/sample - loss: 3.1814 - accuracy: 0.2368\n",
      "Epoch 2/15\n",
      "2500/2500 [==============================] - 1s 384us/sample - loss: 2.8735 - accuracy: 0.2680\n",
      "Epoch 3/15\n",
      "2500/2500 [==============================] - 1s 345us/sample - loss: 2.5235 - accuracy: 0.3736\n",
      "Epoch 4/15\n",
      "2500/2500 [==============================] - 1s 331us/sample - loss: 2.1471 - accuracy: 0.4708\n",
      "Epoch 5/15\n",
      "2500/2500 [==============================] - 1s 326us/sample - loss: 1.9047 - accuracy: 0.5492\n",
      "Epoch 6/15\n",
      "2500/2500 [==============================] - 1s 328us/sample - loss: 1.7340 - accuracy: 0.5932\n",
      "Epoch 7/15\n",
      "2500/2500 [==============================] - 1s 349us/sample - loss: 1.4903 - accuracy: 0.6644\n",
      "Epoch 8/15\n",
      "2500/2500 [==============================] - 1s 331us/sample - loss: 1.3271 - accuracy: 0.7320\n",
      "Epoch 9/15\n",
      "2500/2500 [==============================] - 1s 343us/sample - loss: 1.2107 - accuracy: 0.7652\n",
      "Epoch 10/15\n",
      "2500/2500 [==============================] - 1s 343us/sample - loss: 1.0831 - accuracy: 0.8112\n",
      "Epoch 11/15\n",
      "2500/2500 [==============================] - 1s 342us/sample - loss: 0.9617 - accuracy: 0.8476\n",
      "Epoch 12/15\n",
      "2500/2500 [==============================] - 1s 350us/sample - loss: 0.9053 - accuracy: 0.8676\n",
      "Epoch 13/15\n",
      "2500/2500 [==============================] - 1s 336us/sample - loss: 0.8583 - accuracy: 0.8912\n",
      "Epoch 14/15\n",
      "2500/2500 [==============================] - 1s 395us/sample - loss: 0.8241 - accuracy: 0.9116\n",
      "Epoch 15/15\n",
      "2500/2500 [==============================] - 1s 360us/sample - loss: 0.7475 - accuracy: 0.9328\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a5a90a210>"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_main = Sequential()\n",
    "model_main.add(Dense(1000, input_dim=pc_dim, activation='tanh',kernel_initializer = 'lecun_uniform',activity_regularizer=l1(0.001)))\n",
    "model_main.add(Dense(600, activation='tanh',kernel_initializer = 'lecun_uniform'))\n",
    "model_main.add(Dense(600, activation='tanh',kernel_initializer = 'lecun_uniform'))\n",
    "model_main.add(Dense(600, activation='tanh',kernel_initializer = 'lecun_uniform'))\n",
    "model_main.add(Dense(600, activation='tanh',kernel_initializer = 'lecun_uniform'))\n",
    "model_main.add(Dense(400, activation='tanh',kernel_initializer = 'lecun_uniform'))\n",
    "model_main.add(Dropout(0.2))\n",
    "model_main.add(Dense(22, activation = 'sigmoid'))\n",
    "\n",
    "model_main.summary()\n",
    "\n",
    "model_main.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model_main.fit(X_pca, y, epochs = 15)\n",
    "\n",
    "model_main.save('../output/neural_network.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = load_model(path + 'neural_network.h5')\n",
    "df = pd.read_csv('../data/test_set_sec1/labels_prediction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the model to the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pt_dir = \"../data/test_set_sec1/points/\"\n",
    "# import matlab matrix and round to int\n",
    "ntest_mat = 2500\n",
    "index = ['{0:04}'.format(num) for num in range(1, ntest_mat + 1)]\n",
    "\n",
    "test_mats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in index:\n",
    "    temp = loadmat( test_pt_dir + ind + \".mat\")\n",
    "    test_mats.append(temp[[*temp][-1]])\n",
    "    \n",
    "test_mats = [mat.round() for mat in test_mats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_feature_dist_df = pd.DataFrame(feature_distance(test_mats))\n",
    "t_feature_slope_df = pd.DataFrame(feature_slope(test_mats))\n",
    "t_feature_all_df = pd.concat([t_feature_dist_df, t_feature_slope_df], axis=1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "baseline model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pred = baseGBM_final.predict(t_feature_dist_df)\n",
    "df['Baseline'] = base_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "advanced model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "tX = t_feature_all_df\n",
    "\n",
    "#tpca = PCA(n_components=0.99, whiten=True)\n",
    "tX_pca = pca.transform(tX)\n",
    "print(len(tX_pca[1]))\n",
    "tpc_dim = len(tX_pca[1])\n",
    "\n",
    "#tpred = loaded.predict(tX)\n",
    "#tpred_index = np.argmax(tpred, axis = 1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpred = loaded.predict(tX_pca)\n",
    "tpred_index = np.argmax(tpred, axis = 1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Advanced'] = tpred_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Index</th>\n",
       "      <th>Baseline</th>\n",
       "      <th>Advanced</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Index  Baseline  Advanced\n",
       "0           0      1        12        22\n",
       "1           1      2        17        17\n",
       "2           2      3         9         2\n",
       "3           3      4         5         5\n",
       "4           4      5        16        16"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/test_set_sec1/labels_prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
